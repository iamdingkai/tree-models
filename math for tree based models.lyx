#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Math for Tree-Based Models
\end_layout

\begin_layout Standard
General notations:
\end_layout

\begin_layout Itemize
Training data 
\begin_inset Formula $\{x_{i},y_{i}\}_{i=1}^{N}$
\end_inset


\end_layout

\begin_layout Section
Decision Tree
\end_layout

\begin_layout Itemize
Relatively simple
\end_layout

\begin_layout Itemize
At each split, 
\end_layout

\begin_deeper
\begin_layout Itemize
Chop each feature into 4 quartiles 
\end_layout

\begin_layout Itemize
Trying splitting at each of the quartile
\end_layout

\begin_layout Itemize
Pick the split that generates the lowest entropy
\end_layout

\end_deeper
\begin_layout Section
Random Forest
\end_layout

\begin_layout Itemize
Similar to Decision Tree
\end_layout

\begin_layout Itemize
2 main differences:
\end_layout

\begin_deeper
\begin_layout Itemize
At each split, only look at a random subset of features
\end_layout

\begin_layout Itemize
Bootstrap the original training set n_base_learner times
\end_layout

\begin_layout Itemize
For each bootstrap sample, train a separate tree
\end_layout

\begin_layout Itemize
Take the average of all bootstrap predictions
\end_layout

\end_deeper
\begin_layout Section
AdaBoost
\end_layout

\begin_layout Standard
For each round 
\begin_inset Formula $b=1,2,3,...,B$
\end_inset

, where 
\begin_inset Formula $B=\text{n\_base\_learner}$
\end_inset


\end_layout

\begin_layout Enumerate
Take the training sample 
\begin_inset Formula $\{x_{i}^{b},y_{i}^{b}\}_{i=1}^{N}$
\end_inset

 
\end_layout

\begin_layout Enumerate
When 
\begin_inset Formula $b=1$
\end_inset

, it's the original sample
\end_layout

\begin_deeper
\begin_layout Enumerate
When 
\begin_inset Formula $b\geq1$
\end_inset

, it's a weighted sample (defined below)
\end_layout

\end_deeper
\begin_layout Enumerate
Fit a base learner 
\begin_inset Formula $f_{b}$
\end_inset

: a decision tree (of max_depth = 1)
\end_layout

\begin_layout Enumerate
Check the performance of the base learner 
\begin_inset Formula $f_{b}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Make predictions using 
\begin_inset Formula $f_{b}$
\end_inset

 on the training set 
\begin_inset Formula $\{x_{i}^{b},y_{i}^{b}\}_{i=1}^{N}$
\end_inset

 
\end_layout

\begin_layout Enumerate
Calculate the overall performance of base learner 
\begin_inset Formula $f_{b}$
\end_inset

:
\begin_inset Formula 
\[
\text{error rate}_{b}=\frac{\#\{i:f_{b}(x_{i}^{b})\neq y_{i}\}}{N}
\]

\end_inset


\begin_inset Formula 
\[
\text{amount of say}_{b}=\ln\frac{1-\text{error rate}_{b}}{\text{error rate}_{b}}+\ln(K-1)
\]

\end_inset


\end_layout

\begin_layout Enumerate
Calculate the sample weight for the next round:
\begin_inset Formula 
\[
w_{i}=\frac{1}{N}\times e^{1(y_{i}^{b}\neq f_{b}(x_{i}^{b})\times\text{amount of say}_{b}}
\]

\end_inset

Basically, if 
\begin_inset Formula $f_{b}$
\end_inset

 makes a correct prediction on 
\begin_inset Formula $i$
\end_inset

, then 
\begin_inset Formula $w_{i}=\frac{1}{N}$
\end_inset

.
 If 
\begin_inset Formula $f_{b}$
\end_inset

 makes a wrong prediction on 
\begin_inset Formula $i$
\end_inset

, then we jack up the weight of 
\begin_inset Formula $w_{i}>\frac{1}{N}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
We update the next-round dataset using repeated sampling from 
\begin_inset Formula $\{x_{i},y_{i}\}_{i=1}^{N}$
\end_inset

 using weights 
\begin_inset Formula $\{w_{i}\}$
\end_inset


\end_layout

\begin_layout Standard
Finally, predictions will be based on all base_learners 
\begin_inset Formula $f_{b}$
\end_inset

 of each round, weighted by 
\begin_inset Formula $\text{amount of say}_{b}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Intuitively, this process iteratively increases the weight of observations
 
\begin_inset Formula $i$
\end_inset

 with wrong predictions, so that hopefully model takes better care of those
 segments gradually over time.
\end_layout

\end_body
\end_document
